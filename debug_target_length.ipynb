{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "os.environ['PROJ_DATA'] = \"/pscratch/sd/p/plutzner/proj_data\"\n",
    "import xarray as xr\n",
    "import torch\n",
    "import torchinfo\n",
    "import random\n",
    "import numpy as np\n",
    "import importlib as imp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import cartopy.crs as ccrs\n",
    "import json\n",
    "import pickle\n",
    "import gzip\n",
    "#import matplotlib.colors as mcolorsxx\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import utils\n",
    "import utils.filemethods as filemethods\n",
    "import databuilder.data_loader as data_loader\n",
    "import model.loss as module_loss\n",
    "import model.metric as module_metric\n",
    "from databuilder.data_generator import multi_input_data_organizer\n",
    "import databuilder.data_loader as data_loader\n",
    "from trainer.trainer import Trainer\n",
    "from model.build_model import TorchModel\n",
    "from utils import utils\n",
    "from databuilder.sampleclass import SampleDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check climate data class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'(' was never closed (572498212.py, line 54)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 54\u001b[0;36m\u001b[0m\n\u001b[0;31m    test_ds = test_ds.sel(time = slice(\"1850\", \"2014\")\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m '(' was never closed\n"
     ]
    }
   ],
   "source": [
    "class ClimateData:\n",
    "    \" Custom dataset for climate data and processing \"\n",
    "\n",
    "    def __init__(self, config, expname, seed, data_dir, figure_dir, target_only=False, fetch=True, verbose=False):\n",
    "   \n",
    "        self.config = config\n",
    "        self.expname = expname\n",
    "        self.seed = seed\n",
    "        self.data_dir = data_dir\n",
    "        self.figure_dir = figure_dir\n",
    "        self.verbose = verbose\n",
    "        self.target_only = target_only\n",
    "    \n",
    "        if fetch:\n",
    "            self.fetch_data()\n",
    "\n",
    "    def fetch_data(self, verbose=None):\n",
    "        if verbose is not None: \n",
    "            self.verbose = verbose\n",
    "\n",
    "        self.d_train = SampleDict()\n",
    "        self.d_val = SampleDict()\n",
    "        self.d_test = SampleDict()\n",
    "\n",
    "        self._create_data() \n",
    "\n",
    "        # if self.verbose:\n",
    "        #     self.d_train.summary()\n",
    "        #     self.d_val.summary()\n",
    "        #     self.d_test.summary()\n",
    "\n",
    "        return self.d_train, self.d_val, self.d_test \n",
    "\n",
    "    def _create_data(self):  \n",
    "        for iens, ens in enumerate(self.config[\"ensembles\"]):\n",
    "            print(\"Opening .nc files\")\n",
    "            if self.verbose:\n",
    "                print(ens)\n",
    "            if ens == \"ens1\":   \n",
    "                train_ds = filemethods.get_netcdf_da(self.data_dir + str(ens) + \"/input_vars.v2.LR.historical_0101.eam.h1.1850-2014.nc\")\n",
    "                #train_ds = filemethods.get_netcdf_da(self.data_dir +  \"/input_vars.v2.LR.historical_0101.eam.h1.1850-2014.nc\")\n",
    "\n",
    "            if ens == \"ens2\":\n",
    "                validate_ds = filemethods.get_netcdf_da(self.data_dir + str(ens) + \"/input_vars.v2.LR.historical_0151.eam.h1.1850-2014.nc\")\n",
    "                #validate_ds = filemethods.get_netcdf_da(self.data_dir + \"/input_vars.v2.LR.historical_0151.eam.h1.1850-2014.nc\")\n",
    "\n",
    "            elif ens == \"ens3\":\n",
    "                test_ds = filemethods.get_netcdf_da(self.data_dir + str(ens) + \"/input_vars.v2.LR.historical_0201.eam.h1.1850-2014.nc\")\n",
    "                #test_ds = filemethods.get_netcdf_da(self.data_dir + \"/input_vars.v2.LR.historical_0201.eam.h1.1850-2014.nc\")\n",
    "        \n",
    "  \n",
    "        train_ds = train_ds.sel(time = slice(\"1850\", \"2014\"))\n",
    "        validate_ds = validate_ds.sel(time = slice(\"1850\", \"2014\"))\n",
    "        test_ds = test_ds.sel(time = slice(\"1850\", \"2014\"))\n",
    "\n",
    "        # Get opened X and Y data\n",
    "        # Process Data (compute anomalies)\n",
    "        print(\"Processing training\")\n",
    "        f_dict_train = self._process_data(train_ds)\n",
    "        print(\"Processing validation\")\n",
    "        f_dict_val = self._process_data(validate_ds)\n",
    "        print(\"Processing testing\")\n",
    "        f_dict_test = self._process_data(test_ds)\n",
    "\n",
    "        self.d_train.concat(f_dict_train) \n",
    "        self.d_val.concat(f_dict_val) \n",
    "        self.d_test.concat(f_dict_test) \n",
    "\n",
    "\n",
    "    def _process_data(self, ds):\n",
    "        '''\n",
    "        Motivation: create file data dictionary to contain samples for use in ML model\n",
    "\n",
    "        Input: \n",
    "        - Xarray DataSet\n",
    "            Input dataset contains all input variables in one file\n",
    "\n",
    "        Output: \n",
    "        - Dictionary containing Xarray DataArrays\n",
    "            Output f_dict contains 'da'. \n",
    "            'da' contains multiple dimensions of masked, de-trended, de-seasonalized anomalies for all input variables. \n",
    "            \n",
    "            f_dict contains 'da' using preprocessing keys as pointers\n",
    "\n",
    "        '''\n",
    "\n",
    "        f_dict = SampleDict() \n",
    "\n",
    "        # (1) Isolate the individual dataset values of ds : PRECT, TS, etc. \n",
    "        for ivar, var in enumerate(self.config[\"input_vars\"]):\n",
    "            if ivar == 0:\n",
    "                da = ds[var]\n",
    "                print(\"isolating variables from ds\")\n",
    "                print(f\"da of isolated PRECT shape: {da.shape}\")\n",
    "                if var == \"PRECT\": ## CONVERTING PRECIP TO MM/DAY!\n",
    "                    da = da * 10e3 * 86400 \n",
    "                else:\n",
    "                    pass\n",
    "                da = da.expand_dims(dim={\"channel\": 1}, axis = -1)   # (2) Create a channel dimension in da\n",
    "            else: \n",
    "                da = xr.concat([da, ds[var]], dim = \"channel\")  # (3) Fill channel dim with var array\n",
    "      \n",
    "        da = da.rename('SAMPLES')\n",
    "        da.attrs['long_name'] = None\n",
    "        da.attrs['units'] = None\n",
    "        da.attrs['cell_methods'] = None\n",
    "\n",
    "\n",
    "        # For each input variable or data entity you would like to process: \n",
    "        for ikey, key in enumerate(f_dict):\n",
    "            if key == \"y\":\n",
    "                print(\"Processing target output\")\n",
    "\n",
    "                f_dict[key] = ds[self.config[\"target_var\"]]\n",
    "\n",
    "                if self.config[\"target_var\"] == \"PRECT\": # CONVERTING PRECIP TO MM/DAY!\n",
    "                    f_dict[key] = f_dict[key] * 10e3 * 86400 \n",
    "\n",
    "                print(f\"Length of target after temp conversion = {(len(f_dict[key]))}\")\n",
    "                \n",
    "                # EXTRACT TARGET LOCATION\n",
    "                targetlat = self.config[\"target_region\"][0]\n",
    "                targetlon = self.config[\"target_region\"][1]\n",
    "                f_dict[key] = f_dict[key].sel(lat = targetlat, lon = targetlon, method = 'nearest')\n",
    "\n",
    "                print(f\"Length of target after extract target location = {(len(f_dict[key]))}\")\n",
    "\n",
    "                # REMOVE SEASONAL CYCLE\n",
    "                f_dict[key] = self.trend_remove_seasonal_cycle(f_dict[key])\n",
    "\n",
    "                print(f\"Length of target after trend remove seasonal cycle = {(len(f_dict[key]))}\")\n",
    "\n",
    "                # ROLLING AVERAGE\n",
    "                f_dict[key] = self.rolling_ave(f_dict[key]) # first six values are now nans due to 7-day rolling mean\n",
    "\n",
    "                print(f\"Length of target after rolling average = {(len(f_dict[key]))}\")\n",
    "\n",
    "                # LAG ADJUSTMENT OF TARGET DATASET : Lagging by self.config[\"lagtime\"] number of days allows the input and target samples to align\n",
    "                #  such that each input is paired with a target that is X days in the future\n",
    "                if self.config[\"lagtime\"] != 0: \n",
    "                    f_dict[key] = f_dict[key][ self.config[\"lagtime\"]: ]\n",
    "\n",
    "                print(f\"Length of target after lag adjustment = {(len(f_dict[key]))}\")\n",
    "                 #TODO: Confirm addition of nans?? \"Lead/Lag code for y - shift forward 10 days = input 10x nans at the beginning of the dataset\"\n",
    "\n",
    "            else: \n",
    "                if self.target_only == True:\n",
    "                    pass\n",
    "                else:\n",
    "                    print(\"Processing inputs\")\n",
    "                    if len(self.config[\"input_vars\"]) == 1:\n",
    "                        f_dict[key] = da\n",
    "                    \n",
    "                        ## EXTRACT REGION\n",
    "                        f_dict[key] = self._extractregion(f_dict[key])\n",
    "\n",
    "                        ## MASK LAND/OCEAN \n",
    "                        f_dict[key] = self._masklandocean(f_dict[key])\n",
    "                    \n",
    "                        ## REMOVE SEASONAL CYCLE\n",
    "                        f_dict[key] = self.trend_remove_seasonal_cycle(f_dict[key])\n",
    "\n",
    "                        ## ROLLING AVERAGE \n",
    "                        f_dict[key] = self.rolling_ave(f_dict[key])\n",
    "\n",
    "                        ## LAG ADJUSTMENT OF INPUT: \n",
    "                        f_dict[key] = f_dict[key][0 : -self.config[\"lagtime\"], ...]\n",
    "\n",
    "                    else:\n",
    "                        # LOAD f_dict dictionary with unprocessed channels of 'da'\n",
    "                        f_dict[key] = da \n",
    "                \n",
    "                        ## EXTRACT REGION\n",
    "                        f_dict[key] = self._extractregion(f_dict[key])\n",
    "\n",
    "                        ## MASK LAND/OCEAN \n",
    "                        f_dict[key] = self._masklandocean(f_dict[key])\n",
    "\n",
    "                        # REMOVE SEASONAL CYCLE\n",
    "                        for ichannel in range(f_dict[key].shape[-1]):\n",
    "                            f_dict[key][..., ichannel] = self.trend_remove_seasonal_cycle(f_dict[key][...,ichannel])\n",
    "                        \n",
    "                        # checkplot = f_dict[key].sel(time = '1905-01-01')\n",
    "                        # checkplot[...,1].plot()\n",
    "\n",
    "                        ## ROLLING AVERAGE \n",
    "                        f_dict[key] = self.rolling_ave(f_dict[key])\n",
    "\n",
    "                        ## LAG ADJUSTMENT OF INPUT: \n",
    "                        f_dict[key] = f_dict[key][0 : -self.config[\"lagtime\"], ...]\n",
    "                    \n",
    "                    # Confirmed smoothed, detrended, deseasonalized, lag-adjusted anomalies of PRECT and TS\n",
    "            \n",
    "        return f_dict\n",
    "    \n",
    "    def _extractregion(self, da): \n",
    "        if self.config[\"input_region\"] == \"None\": \n",
    "            \n",
    "            # \"input_region\": [[-15.0, 15.0, 40.0, 300.0],\n",
    "            #              [-15.0, 15.0, 40.0, 300.0]],\n",
    "            \n",
    "            min_lon, max_lon = [0, 360]\n",
    "            min_lat, max_lat = [-90, 90]\n",
    "            print(\"input region is none\")\n",
    "        else:\n",
    "            min_lat, max_lat = self.config[\"input_region\"][:2]\n",
    "            min_lon, max_lon = self.config[\"input_region\"][2:]\n",
    "\n",
    "        if isinstance(da, xr.DataArray):\n",
    "            mask_lon = (da.lon >= min_lon) & (da.lon <= max_lon)\n",
    "            mask_lat = (da.lat >= min_lat) & (da.lat <= max_lat)\n",
    "            data_masked = da.where(mask_lon & mask_lat, drop=True)\n",
    "            return (\n",
    "                data_masked #,\n",
    "                #data_masked[\"lat\"].to_numpy().astype(np.float32),\n",
    "                #data_masked[\"lon\"].to_numpy().astype(np.float32),\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError(\"data must be xarray\")\n",
    "        \n",
    "    \n",
    "    def _masklandocean(self, da):\n",
    "        if self.config[\"input_mask\"][0] == \"None\":\n",
    "            return da\n",
    "        \n",
    "        mask = xr.open_dataset(self.data_dir + \"/landfrac.bilin.nc\")[\"LANDFRAC\"][0, :, :]\n",
    "\n",
    "        if self.config[\"input_mask\"][0] == \"land\":\n",
    "            da_masked = da * xr.where(mask > 0.5, 1.0, 0.0)\n",
    "        elif self.config[\"input_mask\"][0] == \"ocean\":\n",
    "            da_masked = da * xr.where(mask > 0.5, 0.0, 1.0)\n",
    "        else: \n",
    "            raise NotImplementedError('oops NONE error - line 147 of _masklandocean')\n",
    "        \n",
    "        return da_masked\n",
    "\n",
    "    def subtract_trend(self, x): \n",
    "        \n",
    "        detrendOrder = 3\n",
    "\n",
    "        curve = np.polynomial.polynomial.polyfit(np.arange(0, x.shape[0]), x, detrendOrder)\n",
    "        trend = np.polynomial.polynomial.polyval(np.arange(0, x.shape[0]), curve) \n",
    "    \n",
    "        try: \n",
    "            detrend = x - np.swapaxes(trend, 0, 1)\n",
    "        except:\n",
    "            detrend = x - trend\n",
    "        return detrend \n",
    "    \n",
    "    \n",
    "    def trend_remove_seasonal_cycle(self, da):\n",
    "\n",
    "        if len(np.array(da.shape)) == 1: \n",
    "            return da.groupby(\"time.dayofyear\").map(self.subtract_trend).dropna(\"time\")\n",
    "        \n",
    "        else: \n",
    "            da_copy = da.copy()\n",
    "\n",
    "            inc = 45 # 45 degree partitions in longitude to split up the data\n",
    "        \n",
    "            for iloop in np.arange(0, da_copy.shape[2] // inc + 1):\n",
    "                start = inc * iloop\n",
    "                end = np.min([inc * (iloop + 1), da_copy.shape[2]])\n",
    "                if start == end:\n",
    "                    break\n",
    "\n",
    "                stacked = da[:, :, start:end].stack(z=(\"lat\", \"lon\"))\n",
    "\n",
    "                da_copy[:, :, start:end] = stacked.groupby(\"time.dayofyear\").map(self.subtract_trend).unstack()\n",
    "        \n",
    "        return da_copy.dropna(\"time\")\n",
    "\n",
    "    def rolling_ave(self, da):\n",
    "        if self.config[\"averaging_length\"] == 0:\n",
    "            return da\n",
    "        else: \n",
    "            if len(da.shape) == 1: \n",
    "                return da.rolling(time = self.config[\"averaging_length\"]).mean()\n",
    "            else: \n",
    "                da_copy = da.copy()\n",
    "                inc = 45\n",
    "                for iloop in np.arange(0, da.shape[2] // inc + 1): \n",
    "                    start = inc * iloop\n",
    "                    end = np.min([inc *(iloop + 1), da_copy.shape[2]])\n",
    "                    if start == end: \n",
    "                        break\n",
    "\n",
    "                    da_copy[:, :, start:end] = da[:, :, start:end].rolling(time = self.config[\"averaging_length\"]).mean()\n",
    "\n",
    "                return da_copy\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = utils.get_config(\"exp001\")\n",
    "seed = config[\"seed_list\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp.reload(utils)\n",
    "imp.reload(filemethods)\n",
    "\n",
    "data = ClimateData(\n",
    "    config[\"databuilder\"], \n",
    "    expname = config[\"expname\"],\n",
    "    seed=seed,\n",
    "    data_dir = config[\"data_dir\"], \n",
    "    figure_dir=config[\"figure_dir\"],\n",
    "    target_only=True,\n",
    "    fetch=False,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening .nc files\n",
      "Opening .nc files\n",
      "Opening .nc files\n",
      "train ds shape <xarray.DataArray 'time' (time: 60226)> Size: 482kB\n",
      "array([cftime.DatetimeNoLeap(1850, 1, 1, 0, 0, 0, 0, has_year_zero=True),\n",
      "       cftime.DatetimeNoLeap(1850, 1, 2, 0, 0, 0, 0, has_year_zero=True),\n",
      "       cftime.DatetimeNoLeap(1850, 1, 3, 0, 0, 0, 0, has_year_zero=True), ...,\n",
      "       cftime.DatetimeNoLeap(2014, 12, 30, 0, 0, 0, 0, has_year_zero=True),\n",
      "       cftime.DatetimeNoLeap(2014, 12, 31, 0, 0, 0, 0, has_year_zero=True),\n",
      "       cftime.DatetimeNoLeap(2015, 1, 1, 0, 0, 0, 0, has_year_zero=True)],\n",
      "      dtype=object)\n",
      "Coordinates:\n",
      "  * time     (time) object 482kB 1850-01-01 00:00:00 ... 2015-01-01 00:00:00\n",
      "Attributes:\n",
      "    standard_name:  time\n",
      "    long_name:      time\n",
      "    bounds:         time_bnds\n",
      "    axis:           T\n",
      "post slice train ds shape <xarray.DataArray 'time' (time: 60225)> Size: 482kB\n",
      "array([cftime.DatetimeNoLeap(1850, 1, 1, 0, 0, 0, 0, has_year_zero=True),\n",
      "       cftime.DatetimeNoLeap(1850, 1, 2, 0, 0, 0, 0, has_year_zero=True),\n",
      "       cftime.DatetimeNoLeap(1850, 1, 3, 0, 0, 0, 0, has_year_zero=True), ...,\n",
      "       cftime.DatetimeNoLeap(2014, 12, 29, 0, 0, 0, 0, has_year_zero=True),\n",
      "       cftime.DatetimeNoLeap(2014, 12, 30, 0, 0, 0, 0, has_year_zero=True),\n",
      "       cftime.DatetimeNoLeap(2014, 12, 31, 0, 0, 0, 0, has_year_zero=True)],\n",
      "      dtype=object)\n",
      "Coordinates:\n",
      "  * time     (time) object 482kB 1850-01-01 00:00:00 ... 2014-12-31 00:00:00\n",
      "Attributes:\n",
      "    standard_name:  time\n",
      "    long_name:      time\n",
      "    bounds:         time_bnds\n",
      "    axis:           T\n",
      "Processing training\n",
      "isolating variables from ds\n",
      "da of isolated PRECT shape: (60225, 180, 360)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m d_train, d_val, d_test \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 25\u001b[0m, in \u001b[0;36mClimateData.fetch_data\u001b[0;34m(self, verbose)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_val \u001b[38;5;241m=\u001b[39m SampleDict()\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_test \u001b[38;5;241m=\u001b[39m SampleDict()\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# if self.verbose:\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#     self.d_train.summary()\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#     self.d_val.summary()\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m#     self.d_test.summary()\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_train, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_val, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_test\n",
      "Cell \u001b[0;32mIn[7], line 61\u001b[0m, in \u001b[0;36mClimateData._create_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Get opened X and Y data\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Process Data (compute anomalies)\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing training\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 61\u001b[0m f_dict_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing validation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     63\u001b[0m f_dict_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(validate_ds)\n",
      "Cell \u001b[0;32mIn[7], line 98\u001b[0m, in \u001b[0;36mClimateData._process_data\u001b[0;34m(self, ds)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mda of isolated PRECT shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mda\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m var \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPRECT\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;66;03m## CONVERTING PRECIP TO MM/DAY!\u001b[39;00m\n\u001b[0;32m---> 98\u001b[0m     da \u001b[38;5;241m=\u001b[39m \u001b[43mda\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10e3\u001b[39;49m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m86400\u001b[39m \n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/env-torch/lib/python3.10/site-packages/xarray/core/_typed_ops.py:252\u001b[0m, in \u001b[0;36mDataArrayOpsMixin.__mul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__mul__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other: DaCompatible) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[0;32m--> 252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_binary_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/env-torch/lib/python3.10/site-packages/xarray/core/dataarray.py:4680\u001b[0m, in \u001b[0;36mDataArray._binary_op\u001b[0;34m(self, other, f, reflexive)\u001b[0m\n\u001b[1;32m   4676\u001b[0m other_variable_or_arraylike: DaCompatible \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(other, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvariable\u001b[39m\u001b[38;5;124m\"\u001b[39m, other)\n\u001b[1;32m   4677\u001b[0m other_coords \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(other, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoords\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   4679\u001b[0m variable \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 4680\u001b[0m     \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvariable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother_variable_or_arraylike\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4681\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m reflexive\n\u001b[1;32m   4682\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m f(other_variable_or_arraylike, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariable)\n\u001b[1;32m   4683\u001b[0m )\n\u001b[1;32m   4684\u001b[0m coords, indexes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoords\u001b[38;5;241m.\u001b[39m_merge_raw(other_coords, reflexive)\n\u001b[1;32m   4685\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result_name(other)\n",
      "File \u001b[0;32m~/miniconda3/envs/env-torch/lib/python3.10/site-packages/xarray/core/_typed_ops.py:482\u001b[0m, in \u001b[0;36mVariableOpsMixin.__mul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__mul__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other: VarCompatible) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self \u001b[38;5;241m|\u001b[39m T_DataArray:\n\u001b[0;32m--> 482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_binary_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/env-torch/lib/python3.10/site-packages/xarray/core/variable.py:2273\u001b[0m, in \u001b[0;36mVariable._binary_op\u001b[0;34m(self, other, f, reflexive)\u001b[0m\n\u001b[1;32m   2270\u001b[0m attrs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_attrs \u001b[38;5;28;01mif\u001b[39;00m keep_attrs \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2271\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(\u001b[38;5;28mall\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   2272\u001b[0m     new_data \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 2273\u001b[0m         \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mself_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother_data\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m reflexive \u001b[38;5;28;01melse\u001b[39;00m f(other_data, self_data)\n\u001b[1;32m   2274\u001b[0m     )\n\u001b[1;32m   2275\u001b[0m result \u001b[38;5;241m=\u001b[39m Variable(dims, new_data, attrs\u001b[38;5;241m=\u001b[39mattrs)\n\u001b[1;32m   2276\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "d_train, d_val, d_test = data.fetch_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
