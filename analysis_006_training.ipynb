{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python version = 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:12:31) [Clang 14.0.6 ]\n",
      "numpy version = 1.26.4\n",
      "xarray version = 2024.2.0\n",
      "pytorch version = 2.1.2.post2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "os.environ['PROJ_DATA'] = \"/pscratch/sd/p/plutzner/proj_data\"\n",
    "import xarray as xr\n",
    "import torch\n",
    "import torchinfo\n",
    "import random\n",
    "import numpy as np\n",
    "import importlib as imp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import cartopy.crs as ccrs\n",
    "import json\n",
    "import pickle\n",
    "import gzip\n",
    "import scipy\n",
    "from scipy import stats\n",
    "#import matplotlib.colors as mcolorsxx\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import utils\n",
    "import utils.filemethods as filemethods\n",
    "import databuilder.data_loader as data_loader\n",
    "import databuilder.data_generator as data_generator\n",
    "from databuilder.data_generator import ClimateData\n",
    "import model.loss as module_loss\n",
    "import model.metric as module_metric\n",
    "from databuilder.data_generator import multi_input_data_organizer\n",
    "import databuilder.data_loader as data_loader\n",
    "from trainer.trainer import Trainer\n",
    "from model.build_model import TorchModel\n",
    "from base.base_model import BaseModel\n",
    "from utils import utils\n",
    "from shash.shash_torch import Shash\n",
    "# import databuilder.nino_indices as nino_indices # CAUSES CELL TO HANG\n",
    "\n",
    "print(f\"python version = {sys.version}\")\n",
    "print(f\"numpy version = {np.__version__}\")\n",
    "print(f\"xarray version = {xr.__version__}\")\n",
    "print(f\"pytorch version = {torch.__version__}\")\n",
    "\n",
    "# https://github.com/victoresque/pytorch-template/tree/master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract PRECT anomalies time series over Seattle Metro Area (TARGET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = utils.get_config(\"exp006\")\n",
    "seed = config[\"seed_list\"][0]\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "imp.reload(utils)\n",
    "imp.reload(filemethods)\n",
    "imp.reload(data_generator)\n",
    "imp.reload(data_loader)\n",
    "\n",
    "data = ClimateData(\n",
    "    config[\"databuilder\"], \n",
    "    expname = config[\"expname\"],\n",
    "    seed=seed,\n",
    "    data_dir = config[\"perlmutter_data_dir\"], \n",
    "    figure_dir=config[\"perlmutter_output_dir\"],\n",
    "    target_only = True, \n",
    "    fetch=False,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening .nc files\n",
      "Opening .nc files\n"
     ]
    }
   ],
   "source": [
    "# d_train, d_val, d_test = data.fetch_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# d_train[\"y\"][500:540]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# import gzip\n",
    "\n",
    "# target_PRECT_savename1 = \"/pscratch/sd/p/plutzner/E3SM/bigdata/presaved/exp006_d_train_PRECT_1850-2014_unlagged.pkl\"\n",
    "# # target_PRECT_savename1 = \"/Users/C830793391/BIG_DATA/E3SM_Data/presaved/exp006_d_train_SeattleRegional_PRECT_1850-2014_unlagged.pkl\"\n",
    "# with gzip.open(target_PRECT_savename1, \"wb\") as fp:\n",
    "#     pickle.dump(d_train, fp)\n",
    "\n",
    "# target_PRECT_savename2 = \"/pscratch/sd/p/plutzner/E3SM/bigdata/presaved/exp006_d_val_PRECT_1850-2014_unlagged.pkl\"\n",
    "# # target_PRECT_savename2 = \"/Users/C830793391/BIG_DATA/E3SM_Data/presaved/exp006_d_val_SeattleRegional_PRECT_1850-2014_unlagged.pkl\"\n",
    "# with gzip.open(target_PRECT_savename2, \"wb\") as fp:\n",
    "#     pickle.dump(d_val, fp)\n",
    "\n",
    "# target_PRECT_savename3 = \"/pscratch/sd/p/plutzner/E3SM/bigdata/presaved/exp006_d_test_PRECT_1850-2014_unlagged.pkl\"\n",
    "# # target_PRECT_savename3 = \"/Users/C830793391/BIG_DATA/E3SM_Data/presaved/exp006_d_test_SeattleRegional_PRECT_1850-2014_unlagged.pkl\"\n",
    "# with gzip.open(target_PRECT_savename3, \"wb\") as fp:\n",
    "#     pickle.dump(d_test, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process + Pickle Inputs and Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# s_dict_train, s_dict_val, s_dict_test = multi_input_data_organizer(config, MJO = True, ENSO = True, other = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# s_dict_train[\"y\"][500:540]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/pscratch/sd/p/plutzner/E3SM/bigdata/presaved/exp006_train_unlagged_28.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 16\u001b[0m\n\u001b[1;32m     11\u001b[0m s_dict_savename3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperlmutter_data_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpresaved/exp006_test_unlagged_28.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# s_dict_savename3 = '/Users/C830793391/BIG_DATA/E3SM_Data/presaved/Network Inputs/exp006_test_unlagged.pkl'\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# with gzip.open(s_dict_savename3, \"wb\") as fp:\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#     pickle.dump(s_dict_test, fp)\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mgzip\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms_dict_savename1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m obj1:\n\u001b[1;32m     17\u001b[0m     train_dat \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(obj1)\n\u001b[1;32m     18\u001b[0m obj1\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/envs/env-torch/lib/python3.10/gzip.py:58\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(filename, mode, compresslevel, encoding, errors, newline)\u001b[0m\n\u001b[1;32m     56\u001b[0m gz_mode \u001b[38;5;241m=\u001b[39m mode\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filename, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike)):\n\u001b[0;32m---> 58\u001b[0m     binary_file \u001b[38;5;241m=\u001b[39m \u001b[43mGzipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgz_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompresslevel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     60\u001b[0m     binary_file \u001b[38;5;241m=\u001b[39m GzipFile(\u001b[38;5;28;01mNone\u001b[39;00m, gz_mode, compresslevel, filename)\n",
      "File \u001b[0;32m~/miniconda3/envs/env-torch/lib/python3.10/gzip.py:174\u001b[0m, in \u001b[0;36mGzipFile.__init__\u001b[0;34m(self, filename, mode, compresslevel, fileobj, mtime)\u001b[0m\n\u001b[1;32m    172\u001b[0m     mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fileobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     fileobj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmyfileobj \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    176\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(fileobj, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/pscratch/sd/p/plutzner/E3SM/bigdata/presaved/exp006_train_unlagged_28.pkl'"
     ]
    }
   ],
   "source": [
    "s_dict_savename1 = str(config[\"perlmutter_data_dir\"]) + 'presaved/exp006_train_unlagged_28.pkl'\n",
    "# s_dict_savename1 = '/Users/C830793391/BIG_DATA/E3SM_Data/presaved/Network Inputs/exp006_train_unlagged.pkl'\n",
    "# with gzip.open(s_dict_savename1, \"wb\") as fp:\n",
    "#     pickle.dump(s_dict_train, fp)\n",
    "\n",
    "s_dict_savename2 = str(config[\"perlmutter_data_dir\"]) + 'presaved/exp006_val_unlagged_28.pkl'\n",
    "# s_dict_savename2 = '/Users/C830793391/BIG_DATA/E3SM_Data/presaved/Network Inputs/exp006_val_unlagged.pkl'\n",
    "# with gzip.open(s_dict_savename2, \"wb\") as fp:\n",
    "#     pickle.dump(s_dict_val, fp)\n",
    "\n",
    "s_dict_savename3 = str(config[\"perlmutter_data_dir\"]) + 'presaved/exp006_test_unlagged_28.pkl'\n",
    "# s_dict_savename3 = '/Users/C830793391/BIG_DATA/E3SM_Data/presaved/Network Inputs/exp006_test_unlagged.pkl'\n",
    "# with gzip.open(s_dict_savename3, \"wb\") as fp:\n",
    "#     pickle.dump(s_dict_test, fp)\n",
    "\n",
    "with gzip.open(s_dict_savename1, \"rb\") as obj1:\n",
    "    train_dat = pickle.load(obj1)\n",
    "obj1.close()\n",
    "\n",
    "with gzip.open(s_dict_savename2, \"rb\") as obj2:\n",
    "    val_dat = pickle.load(obj2)\n",
    "obj2.close()\n",
    "\n",
    "with gzip.open(s_dict_savename3, \"rb\") as obj3:\n",
    "    test_dat = pickle.load(obj3)\n",
    "obj3.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dat[\"y\"][500:530]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(np.isnan(train_dat[\"x\"][121:-32]).any())\n",
    "print(np.isnan(val_dat[\"x\"][121:-32]).any())\n",
    "print(np.isnan(test_dat[\"x\"][121:-32]).any())\n",
    "\n",
    "print(np.isnan(train_dat[\"y\"][121:-32]).any())\n",
    "print(np.isnan(val_dat[\"y\"][121:-32]).any())\n",
    "print(np.isnan(test_dat[\"y\"][121:-32]).any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve Data: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opened pickle file: /pscratch/sd/p/plutzner/E3SM/bigdata/presaved/exp006_train_unlagged_28.pkl\n",
      "trimmed data shape: (60058,)\n",
      "X shape: (60023, 3)\n",
      "Target shape: (60023,)\n",
      "Opened pickle file: /pscratch/sd/p/plutzner/E3SM/bigdata/presaved/exp006_val_unlagged_28.pkl\n",
      "trimmed data shape: (60058,)\n",
      "X shape: (60023, 3)\n",
      "Target shape: (60023,)\n",
      "Opened pickle file: /pscratch/sd/p/plutzner/E3SM/bigdata/presaved/exp006_test_unlagged_28.pkl\n",
      "trimmed data shape: (60058,)\n",
      "X shape: (60023, 3)\n",
      "Target shape: (60023,)\n"
     ]
    }
   ],
   "source": [
    "# Setup the Data\n",
    "lagtime = config[\"databuilder\"][\"lagtime\"] \n",
    "smoothing_length = config[\"databuilder\"][\"averaging_length\"]\n",
    "\n",
    "trainset = data_loader.CustomData(config[\"data_loader\"][\"perlmutter_data_dir\"] + \"exp006_train_unlagged_28.pkl\", lagtime, smoothing_length)\n",
    "valset = data_loader.CustomData(config[\"data_loader\"][\"perlmutter_data_dir\"] + \"exp006_val_unlagged_28.pkl\", lagtime, smoothing_length)\n",
    "testset = data_loader.CustomData(config[\"data_loader\"][\"perlmutter_data_dir\"] + \"exp006_test_unlagged_28.pkl\", lagtime, smoothing_length)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    trainset,\n",
    "    batch_size=config[\"data_loader\"][\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    valset,\n",
    "    batch_size=config[\"data_loader\"][\"batch_size\"],\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: MPS device not found.Training will be performed on CPU.\n"
     ]
    }
   ],
   "source": [
    "# Setup the Model\n",
    "model = TorchModel(\n",
    "    config=config[\"arch\"],\n",
    "    target_mean=trainset.target.mean(axis=0),\n",
    "    target_std=trainset.target.std(axis=0),\n",
    ")\n",
    "model.freeze_layers(freeze_id=\"tau\")\n",
    "optimizer = getattr(torch.optim, config[\"optimizer\"][\"type\"])(\n",
    "    model.parameters(), **config[\"optimizer\"][\"args\"]\n",
    ")\n",
    "criterion = getattr(module_loss, config[\"criterion\"])()\n",
    "metric_funcs = [getattr(module_metric, met) for met in config[\"metrics\"]]\n",
    "\n",
    "# Build the trainer\n",
    "device = utils.prepare_device(config[\"device\"])\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    criterion,\n",
    "    metric_funcs,\n",
    "    optimizer,\n",
    "    max_epochs=config[\"trainer\"][\"max_epochs\"],\n",
    "    data_loader=train_loader,\n",
    "    validation_data_loader=val_loader,\n",
    "    device=device,\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0/4000\n",
      "  3.2s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch   1/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch   2/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch   3/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch   4/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch   5/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch   6/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch   7/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch   8/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch   9/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  10/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  11/4000\n",
      "  3.2s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  12/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  13/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  14/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  15/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  16/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  17/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  18/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  19/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  20/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  21/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  22/4000\n",
      "  3.2s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  23/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  24/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  25/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  26/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  27/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  28/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  29/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  30/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  31/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  32/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  33/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  34/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  35/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  36/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  37/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  38/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  39/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  40/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  41/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  42/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  43/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  44/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  45/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  46/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  47/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  48/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  49/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  50/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  51/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  52/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  53/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  54/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  55/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  56/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  57/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  58/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  59/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  60/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  61/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  62/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  63/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  64/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  65/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  66/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  67/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  68/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  69/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  70/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  71/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  72/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  73/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  74/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  75/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  76/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  77/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  78/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  79/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  80/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  81/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  82/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  83/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  84/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  85/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  86/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  87/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  88/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  89/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  90/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  91/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  92/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  93/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  94/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  95/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  96/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  97/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  98/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch  99/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 100/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 101/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 102/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 103/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 104/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 105/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 106/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 107/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 108/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 109/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 110/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 111/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 112/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 113/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 114/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 115/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 116/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 117/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 118/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 119/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 120/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 121/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 122/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 123/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 124/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 125/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 126/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 127/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 128/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 129/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 130/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 131/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 132/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 133/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 134/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 135/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 136/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 137/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 138/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 139/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 140/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 141/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 142/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 143/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 144/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 145/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 146/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 147/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 148/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 149/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 150/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 151/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 152/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 153/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 154/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 155/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 156/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 157/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 158/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 159/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 160/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 161/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 162/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 163/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 164/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 165/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 166/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 167/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 168/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 169/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 170/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 171/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 172/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 173/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 174/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 175/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 176/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 177/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 178/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 179/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 180/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 181/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 182/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 183/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 184/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 185/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 186/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 187/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 188/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 189/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 190/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 191/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 192/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 193/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 194/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 195/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 196/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 197/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 198/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n",
      "Epoch 199/4000\n",
      "  3.1s - loss: 16.11810 - val_loss: 16.11810\n"
     ]
    }
   ],
   "source": [
    "# # # Visualize the model\n",
    "# torchinfo.summary(\n",
    "#     model,\n",
    "#     [   trainset.input[: config[\"data_loader\"][\"batch_size\"]].shape ],\n",
    "#     verbose=1,\n",
    "#     col_names=(\"input_size\", \"output_size\", \"num_params\"),\n",
    "# )\n",
    "\n",
    "# Train the Model\n",
    "model.to(device)\n",
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the Model\n",
    "# path = '/Users/C830793391/Documents/Research/E3SM/saved/models/exp006_RERUN_Nov2024.pth'\n",
    "path = str(config[\"perlmutter_model_dir\"]) + 'exp006_RERUN_28.pth'\n",
    "torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the Model\n",
    "# path = '/Users/C830793391/Documents/Research/E3SM/saved/models/exp006_RERUN_Nov2024.pth'\n",
    "path = str(config[\"perlmutter_model_dir\"]) + 'exp006_RERUN_28.pth'\n",
    "model = TorchModel(\n",
    "    config=config[\"arch\"],\n",
    "    target_mean=testset.target.mean(axis=0),\n",
    "    target_std=testset.target.std(axis=0),\n",
    ")\n",
    "model.load_state_dict(torch.load(path))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(trainer.log.history.keys())\n",
    "\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i, m in enumerate((\"loss\", *config[\"metrics\"])):\n",
    "    plt.subplot(1, 4, i + 1)\n",
    "    plt.plot(trainer.log.history[\"epoch\"], trainer.log.history[m], label=m)\n",
    "    plt.plot(\n",
    "        trainer.log.history[\"epoch\"], trainer.log.history[\"val_\" + m], label=\"val_\" + m\n",
    "    )\n",
    "    plt.axvline(\n",
    "       x=trainer.early_stopper.best_epoch, linestyle=\"--\", color=\"k\", linewidth=0.75\n",
    "    )\n",
    "    plt.title(m)\n",
    "    plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(config[\"perlmutter_figure_dir\"] + str(config[\"expname\"]) + \"/training_metrics_exp006_RERUN_28daylag.png\", format = 'png', dpi = 300) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Predictions Against Climatology: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    print(device)\n",
    "    output = model.predict(dataset=testset, batch_size=128, device=device) # The output is the batched SHASH distribution parameters\n",
    "output[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save Model Outputs\n",
    "model_output_pred = str(config[\"perlmutter_output_dir\"]) + 'exp006_output_pred_testset_RERUN_28_frozenTau.pkl'\n",
    "with gzip.open(model_output_pred, \"wb\") as fp:\n",
    "    pickle.dump(output, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Open Model Outputs\n",
    "model_output_pred = str(config[\"perlmutter_output_dir\"]) + 'exp006_output_pred_testset_RERUN_28_frozenTau.pkl'\n",
    "with gzip.open(model_output_pred, \"rb\") as obj1:\n",
    "    output = pickle.load(obj1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import analysis.calc_climatology as calc_climatology\n",
    "from analysis import analysis_metrics\n",
    "\n",
    "exp006_original_output = filemethods.open_data_file('/pscratch/sd/p/plutzner/E3SM/saved/output/exp006_output_pred_testset.pkl')\n",
    "\n",
    "lagtime = config[\"databuilder\"][\"lagtime\"] \n",
    "smoothing_length = config[\"databuilder\"][\"averaging_length\"]  \n",
    "\n",
    "# Open Target Data\n",
    "# target = xr.open_dataset('/Users/C830793391/BIG_DATA/E3SM_Data/presaved/Network Inputs/' + str(config[\"expname\"]) + '_d_test_1850-1900.nc')\n",
    "target = filemethods.open_data_file(config[\"perlmutter_data_dir\"] + 'presaved/exp006_test_unlagged_28.pkl')\n",
    "target = target[\"y\"][lagtime:]\n",
    "target = target[smoothing_length:]\n",
    "\n",
    "# Open Climatology Data: TRAINING DATA\n",
    "# climatology_filename = '/Users/C830793391/BIG_DATA/E3SM_Data/presaved/Network Inputs/' + str(config[\"expname\"]) + '_d_train_1850-1900.nc'\n",
    "climatology_filename = str(config[\"perlmutter_data_dir\"]) + 'presaved/exp006_train_unlagged_28.pkl'\n",
    "climatology_da = analysis_metrics.load_pickle(climatology_filename)\n",
    "climatology = climatology_da[\"y\"][lagtime:]\n",
    "climatology = climatology[smoothing_length:]\n",
    "\n",
    "# Compare SHASH predictions to climatology histogram\n",
    "x = np.arange(-15, 15, 0.01)\n",
    "\n",
    "p = calc_climatology.deriveclimatology(exp006_original_output, climatology, x, number_of_samples=50, config=config, climate_data = climatology_filename)\n",
    "\n",
    "p = calc_climatology.deriveclimatology(output, climatology, x, number_of_samples=50, config=config, climate_data = climatology_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRPS - Compare Distributions against Target / Climatology  ------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proper Scoring CRPS Calculations: \n",
    "https://github.com/properscoring/properscoring/blob/master/properscoring/_crps.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('/Users/C830793391/Documents/Research/E3SM/saved/output/exp006_CRPS_testset_allsamples_V2.pkl', \"rb\") as obj1:\n",
    "    crps_scores = pickle.load(obj1)\n",
    "\n",
    "print(f\" The Mean CRPS score is: {np.round(crps_scores.mean(), 4)}\")\n",
    "print(crps_scores.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Climatology as Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('/Users/C830793391/Documents/Research/E3SM/saved/output/exp006_CRPS_climatology_testset_allsamples_V2.pkl', \"rb\") as obj1:\n",
    "    crps_climatology_scores = pickle.load(obj1)\n",
    "\n",
    "print(crps_climatology_scores.shape)\n",
    "\n",
    "print(f\" The Mean Climatology CRPS score is: {np.round(crps_climatology_scores.mean(), 4)}\")\n",
    "print(f\"Median Climatology CRPS score is: {np.round(np.median(crps_climatology_scores), 4)}\")\n",
    "climatology_median = np.median(crps_climatology_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Calculate the mean CRPS scores for the forecast and climatology\n",
    "CRPS_forecast = np.round(np.mean(crps_scores), 4)\n",
    "CRPS_climatology = np.round(np.mean(crps_climatology_scores), 4)\n",
    "\n",
    "# Plot CRPS comparison\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "ax.scatter(np.arange(0, len(crps_scores)), crps_scores, alpha = 0.4, s=0.4, color ='#26828e', label = f'Forecast CRPS Average: {CRPS_forecast} ')\n",
    "ax.scatter(np.arange(0, len(crps_climatology_scores)), crps_climatology_scores,  alpha = 0.4, s=0.4, color ='#f5a962', label = f'Climatology CRPS Average: {CRPS_climatology} ')\n",
    "ax.set_title(\"CRPS Comparison\")\n",
    "ax.set_xlabel(\"Sample Index\")\n",
    "ax.set_ylabel(\"CRPS Score\")\n",
    "ax.legend(markerscale = 9)\n",
    "plt.savefig('/Users/C830793391/Documents/Research/E3SM/figures/exp006/CRPS_comparison_exp006_RERUN.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort Data into Phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis.ENSO_indices_calculator import identify_nino_phases\n",
    "# Gather ENSO index for *each* sample\n",
    "#(https://climatedataguide.ucar.edu/climate-data/nino-sst-indices-nino-12-3-34-4-oni-and-tni)\n",
    "\n",
    "# Nino34 = testset.input[:,2] # From loaded network inputs above (testset(RMM1, RMM2, NINO34))\n",
    "\n",
    "monthlyENSO = xr.open_dataset('/Users/C830793391/BIG_DATA/E3SM_Data/presaved/ENSO_ne30pg2_HighRes/nino.member0201.nc')\n",
    "Nino34 = monthlyENSO.nino34.values\n",
    "\n",
    "enso_indices_daily = identify_nino_phases(Nino34, threshold = 0.4, window = 6, front_cutoff = front_cutoff, back_cutoff = back_cutoff)\n",
    "# Isolate non-zero indices for each ENSO phase\n",
    "maxnino =  max(np.where(enso_indices_daily[:,0] != 0)[0])\n",
    "maxneutral = max(np.where(enso_indices_daily[:,2] != 0)[0])\n",
    "print(maxneutral)\n",
    "elnino = enso_indices_daily[:17321, 0]\n",
    "lanina = enso_indices_daily[:17010, 1]\n",
    "non_neutral = np.concatenate((elnino, lanina))\n",
    "neutral = np.setdiff1d(np.arange(0, 60225), non_neutral)[:(len(crps_scores) - (len(elnino) + len(lanina)))]\n",
    "\n",
    "CRPS_elnino = np.round(crps_scores[elnino].mean(), 4)\n",
    "CRPS_lanina = np.round(crps_scores[lanina].mean(), 4)\n",
    "CRPS_neutral = np.round(crps_scores[neutral].mean(), 4)\n",
    "\n",
    "# Plot CRPS by ENSO index\n",
    "# create a subplot with three columns and one row\n",
    "fig, ax = plt.subplots(3, 1, figsize=(8, 10), sharey=True)\n",
    "ax[0].scatter(elnino, crps_scores[elnino], s=0.4, color ='#26828e', label = f'CRPS Average: {CRPS_elnino} ')\n",
    "ax[0].set_title('El Nino')\n",
    "ax[0].set_ylabel('CRPS')\n",
    "ax[1].scatter(lanina, crps_scores[lanina], s=0.4, color = '#26828e', label = f'CRPS Average: {CRPS_lanina}')\n",
    "ax[1].set_title('La Nina')\n",
    "ax[1].set_ylabel('CRPS')\n",
    "ax[2].scatter(neutral, crps_scores[neutral], s=0.4, color = '#26828e', label = f'CRPS Average: {CRPS_neutral}')\n",
    "ax[2].set_title('Neutral')\n",
    "ax[2].set_xlabel('Time (Samples in Chronological Order)')\n",
    "ax[2].set_ylabel('CRPS')\n",
    "ax[0].legend(loc = 'upper right')\n",
    "ax[1].legend(loc = 'upper right') \n",
    "ax[2].legend(loc = 'upper right')\n",
    "plt.subplots_adjust(hspace=0.3)\n",
    "\n",
    "print(f\"El Nino average CRPS across all samples: {np.round(crps_scores[elnino].mean(), 4)}\")\n",
    "print(f\"La Nina average CRPS across all samples: {np.round(crps_scores[lanina].mean(), 4)}\")\n",
    "print(f\"Neutral average CRPS across all samples: {np.round(crps_scores[neutral].mean(), 4)}\")\n",
    "\n",
    "plt.savefig('/Users/C830793391/Documents/Research/E3SM/visuals/CRPS_vs_ENSO_Phases_exp006_allsamples.png', format='png', bbox_inches ='tight', dpi = 300)\n",
    "\n",
    "# Weighted average confirmation: \n",
    "CRPS_weightedmean = (CRPS_elnino*elnino.shape[0] + CRPS_lanina*lanina.shape[0] + CRPS_neutral*neutral.shape[0])/(elnino.shape[0] + lanina.shape[0] + neutral.shape[0])\n",
    "print(f\"Weighted average CRPS across all samples: {np.round(CRPS_weightedmean, 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discard Plot : CRPS vs IQR Percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.metric import iqr_basic\n",
    "\n",
    "target = testset.target\n",
    "\n",
    "print(\"output type:\", type(output))\n",
    "print(\"output shape:\", output.shape if hasattr(output, 'shape') else \"N/A\")\n",
    "print(\"testset type:\", type(target))\n",
    "print(\"testset shape:\", target.shape if hasattr(target, 'shape') else \"N/A\")\n",
    "\n",
    "# iqr capture relies on SHASH output parameters (mu, sigma, tau, gamma) and the SHASH class\n",
    "iqr = iqr_basic(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(iqr, crps_scores, 'o', markersize=1)\n",
    "plt.xlabel('Interquartile Range (IQR)')\n",
    "plt.ylabel('CRPS Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an array of averaged CRPS score for each bin of IQR percentile (100, 95, 90, 85, etc.)\n",
    "percentiles = np.linspace(100, 0, 101)\n",
    "\n",
    "avg_crps = []\n",
    "avg_target = []\n",
    "sample_index = np.zeros((len(target), len(percentiles)))\n",
    "for ip, p in enumerate(percentiles):\n",
    "    avg_crps.append(np.mean(crps_scores[iqr < np.percentile(iqr, p)]))\n",
    "    avg_target.append(np.mean(target[iqr < np.percentile(iqr, p)]))\n",
    "    # capture the index (out of 60058) for all the samples in each bin\n",
    "    indices = np.where(iqr < np.percentile(iqr, p))[0]\n",
    "    sample_index[:len(indices), ip] = indices\n",
    "\n",
    "color = 'tab:blue'\n",
    "fig, ax1 = plt.subplots()\n",
    "plt.gca().invert_xaxis()\n",
    "ax1.set_ylabel('Average CRPS')\n",
    "ax1.set_xlabel('IQR Percentile (% Data Remaining)', color=color)\n",
    "ax1.plot(percentiles, avg_crps, color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "ax1.axhline(y=crps_climatology_scores.mean(), color='grey', linestyle='--', label='Climatology Mean')\n",
    "ax1.set_ylim([1.05, 1.205])\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:olive'\n",
    "ax2.set_ylabel('Average Target Anomalies (mm/day)', color=color)\n",
    "ax2.plot(percentiles, avg_target, color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "plt.savefig('/Users/C830793391/Documents/Research/E3SM/visuals/CRPS_IQR_DiscardPlot_fine_mean_V2.png', format='png', bbox_inches ='tight', dpi = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify actual target anomolous precipitation data : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all raw precip anaomaly data: \n",
    "# extract from netcdf:\n",
    "nc_file = xr.open_dataset('/Users/C830793391/BIG_DATA/E3SM_Data/ens3/PRECT.v2.LR.historical_0201.eam.h1.1850-2014.nc')\n",
    "prect_global = nc_file.PRECT\n",
    "\n",
    "min_lat, max_lat = config[\"databuilder\"][\"target_region\"][:2]\n",
    "min_lon, max_lon = config[\"databuilder\"][\"target_region\"][2:]\n",
    "\n",
    "if isinstance(prect_global, xr.DataArray):\n",
    "    mask_lon = (prect_global.lon >= min_lon) & (prect_global.lon <= max_lon)\n",
    "    mask_lat = (prect_global.lat >= min_lat) & (prect_global.lat <= max_lat)\n",
    "    prect_regional = prect_global.where(mask_lon & mask_lat, drop=True)\n",
    "\n",
    "prect_regional = prect_regional.mean(dim=['lat', 'lon']).values[front_cutoff + 7 : - (back_cutoff+8)]\n",
    "print(f\"prect_regional shape: {prect_regional.shape}\")\n",
    "\n",
    "# Convert to mm/day\n",
    "target_raw = prect_regional * 86400 * 1000\n",
    "\n",
    "print(np.mean(target_raw))\n",
    "print(np.median(target_raw))\n",
    "print(np.std(target_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample indices for IQR: \n",
    "def samples_by_IQR(sample_indices, percentile, quantity): \n",
    "    narrow_indices = sample_indices[:, 100-percentile]\n",
    "    # eliminate zero-values from indices:\n",
    "    narrow_indices = narrow_indices[narrow_indices != 0]\n",
    "    narrow_indices = narrow_indices.astype(int)\n",
    "\n",
    "    narrow_values = quantity[narrow_indices]\n",
    "    return narrow_values, narrow_indices\n",
    "\n",
    "# Narrowest 15% IQR for Target Anomalies: \n",
    "narrow_10P_anoms, index10P_anoms = samples_by_IQR(sample_index, 10, target)\n",
    "narrow_10P_anoms_med = np.median(narrow_10P_anoms)\n",
    "narrow_10P_anoms_mean = np.mean(narrow_10P_anoms)\n",
    "print(f\"Median of 10% IQR Target Anomalies: {narrow_10P_anoms_med}\")\n",
    "print(f\"Mean of 10% IQR Target Anomalies: {narrow_10P_anoms_mean}\")\n",
    "print(f\"Standard Deviation of 10% IQR Target Anomalies: {np.std(narrow_10P_anoms)}\\n\")\n",
    "\n",
    "# Narrowest 15% IQR for True Target Values: \n",
    "narrow_10P_true, index10P_true = samples_by_IQR(sample_index, 10, target_raw)\n",
    "narrow_10P_true_med = np.median(narrow_10P_true)\n",
    "narrow_10P_true_mean = np.mean(narrow_10P_true)\n",
    "print(f\"Median of 10% IQR True Target Values: {narrow_10P_true_med}\")\n",
    "print(f\"Mean of 10% IQR True Target Values: {narrow_10P_true_mean}\")\n",
    "print(f\"Standard Deviation of 10% IQR True Target Values: {np.std(narrow_10P_true)}\\n\")\n",
    "\n",
    "# Most Accruate CRPS - Target Anomalies: \n",
    "narrow_20P_anoms, index20P_anoms = samples_by_IQR(sample_index, 20, target)\n",
    "narrow_20P_anoms_med = np.median(narrow_20P_anoms)\n",
    "narrow_20P_anoms_mean = np.mean(narrow_20P_anoms)\n",
    "print(f\"Median of 20% IQR Target Anomalies: {narrow_20P_anoms_med}\")\n",
    "print(f\"Mean of 20% IQR Target Anomalies: {narrow_20P_anoms_mean}\")\n",
    "print(f\"Standard Deviation of 20% IQR Target Anomalies: {np.std(narrow_20P_anoms)}\\n\")\n",
    "\n",
    "# Most Accurate CRPS - True Target Values:\n",
    "narrow_20P_true, index20P_true = samples_by_IQR(sample_index, 20, target_raw)\n",
    "narrow_20P_true_med = np.median(narrow_20P_true)\n",
    "narrow_20P_true_mean = np.mean(narrow_20P_true)\n",
    "print(f\"Median of 20% IQR True Target Values: {narrow_20P_true_med}\")\n",
    "print(f\"Mean of 20% IQR True Target Values: {narrow_20P_true_mean}\")\n",
    "print(f\"Standard Deviation of 20% IQR True Target Values: {np.std(narrow_20P_true)}\\n\")\n",
    "\n",
    "# Most Accruate CRPS - Target Anomalies: \n",
    "narrow_40P_anoms, index40P_anoms = samples_by_IQR(sample_index, 40, target)\n",
    "narrow_40P_anoms_med = np.median(narrow_40P_anoms)\n",
    "narrow_40P_anoms_mean = np.mean(narrow_40P_anoms)\n",
    "print(f\"Median of 40% IQR Target Anomalies: {narrow_40P_anoms_med}\")\n",
    "print(f\"Mean of 40% IQR Target Anomalies: {narrow_40P_anoms_mean}\")\n",
    "print(f\"Standard Deviation of 40% IQR Target Anomalies: {np.std(narrow_40P_anoms)}\\n\")\n",
    "\n",
    "# Most Accurate CRPS - True Target Values:\n",
    "narrow_40P_true, index40P_true = samples_by_IQR(sample_index, 40, target_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(target_raw, bins=100, alpha=0.5, label='Raw Precipitation Data (mm/day)', color = '#b5de2b')\n",
    "plt.hist(narrow_40P_true, bins=100, alpha = 0.4,  label='Narrowest 40% IQR True Target Values (mm/day) (LOWEST CRPS)', color = '#21918c')\n",
    "plt.hist(narrow_20P_true, bins=100, alpha = 0.4,  label='Narrowest 20% IQR True Target Values (mm/day) (LOWEST CRPS)', color = '#3b528b')\n",
    "plt.hist(narrow_10P_true, alpha = 0.4, bins=100, label='Narrowest 10% IQR True Target Values (mm/day)', color = 'purple')\n",
    "plt.xlabel('True Precipitation Amounts (mm/day)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlim(-0.1, 35)\n",
    "plt.legend()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(target_raw, bins=100, alpha=0.5, density = True, label='Raw Precipitation Data (mm/day)', color = '#b5de2b')\n",
    "plt.hist(narrow_40P_true, bins=100, density = True, alpha = 0.4,  label='Narrowest 40% IQR True Target Values (mm/day) (LOWEST CRPS)', color = '#21918c')\n",
    "plt.hist(narrow_20P_true, bins=100,density = True,  alpha = 0.4,  label='Narrowest 20% IQR True Target Values (mm/day) (LOWEST CRPS)', color = '#3b528b')\n",
    "plt.hist(narrow_10P_true, density = True, alpha = 0.4, bins=100, label='Narrowest 10% IQR True Target Values (mm/day)', color = 'purple')\n",
    "plt.xlabel('True Precipitation Amounts (mm/day)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.ylim(0, .2)\n",
    "# plt.xlim(0, 20)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4.6))\n",
    "plt.hist(target, bins=100, alpha=0.5, density = True, label='All Precipitation Anomalies (mm/day)', color = '#b5de2b')\n",
    "plt.hist(narrow_40P_anoms, bins=100, density = True, alpha=0.4,  label='Narrowest 40% IQR Target Anomalies (mm/day) LOWEST CRPS', color = '#21918c')\n",
    "plt.axvline(x=narrow_40P_anoms_med, color='#21918c', linewidth = 0.8, linestyle='--', label='Mean Narrowest 40% IQR Target Anomalies')\n",
    "plt.hist(narrow_20P_anoms, bins=100, density = True, alpha=0.4,  label='Narrowest 20% IQR Target Anomalies (mm/day) LOWEST CRPS', color = '#3b528b')\n",
    "plt.axvline(x=narrow_20P_anoms_med, color='#3b528b', linewidth = 0.8, linestyle='--', label='Mean Narrowest 20% IQR Target Anomalies')\n",
    "plt.hist(narrow_10P_anoms, bins=100, density = True, alpha=0.5, label='Narrowest 10% IQR Target Anomalies (mm/day)', color = 'purple')\n",
    "plt.axvline(x=narrow_10P_anoms_med, color='purple', linewidth = 0.8, linestyle='--', label='Mean Narrowest 10% IQR Target Anomalies')\n",
    "plt.legend(bbox_to_anchor=(1.18, 1), loc='upper right')\n",
    "plt.xlabel('Precipitation Anomalies (mm/day)')\n",
    "plt.ylabel('Normalized Frequency')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elnino_dots = np.zeros(elnino.shape)\n",
    "lanina_dots = np.zeros(lanina.shape)\n",
    "\n",
    "# Scatter Compare: \n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.scatter(sample_index[:,0], target, label = 'All target Anomalies (mm/day)', s=.5, color = '#3b528b')\n",
    "plt.scatter(index10P_anoms, narrow_10P_anoms, label='Narrowest 10% IQR Target Anomalies (mm/day)', s=.5, color = '#5ec962')\n",
    "plt.scatter(elnino, elnino_dots, label='El Nino Events', s=.5, color = '#fdca26')\n",
    "plt.xlabel('Time \\n (Daily Samples in Chronological Order)')\n",
    "plt.ylabel('Precipitation Anomalies (mm/day)')\n",
    "plt.legend(markerscale = 3.5, loc = 'upper right')\n",
    "plt.ylim(-7, 22)\n",
    "\n",
    "plt.savefig('/Users/C830793391/Documents/Research/E3SM/visuals/narrowest10P_anoms_scatter_compare_ENSO_V2.png', format='png', bbox_inches ='tight', dpi = 300)\n",
    "\n",
    "print(target[elnino].shape)\n",
    "\n",
    "\n",
    "# Percent of narrowest IQR Target anomalies that occur during El Nino events:\n",
    "narrow10_during_elnino = np.intersect1d(index10P_anoms, elnino)\n",
    "narrow10_during_elnino_percent = (narrow10_during_elnino.shape[0]/index10P_anoms.shape[0])*100\n",
    "print(f\"Percent of narrowest 10% IQR Target Anomalies that occur during El Nino events: {narrow10_during_elnino_percent}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter Compare: \n",
    "plt.figure(figsize=(13, 4))\n",
    "plt.scatter(sample_index[neutral,0], target[neutral], alpha = 0.8, label = 'Neutral target Anomalies (mm/day)', s=0.1, color = '#b0b0b0')\n",
    "plt.scatter(sample_index[elnino,0], target[elnino], alpha = 0.8, label = 'El Nino target Anomalies (mm/day)', s=0.1, color = '#648FFF')\n",
    "plt.scatter(sample_index[lanina,0], target[lanina], alpha = 0.8, label = 'La Nina target Anomalies (mm/day)', s=0.1, color = '#FFB000')\n",
    "# plt.scatter(index10P_anoms, narrow_10P_anoms, label='Narrowest 10% IQR Target Anomalies (mm/day)', s=1.5, color = '#5ec962')\n",
    "# plt.scatter(elnino, elnino_dots, label='El Nino Events', s=1, color = '#fdca26')\n",
    "plt.xlabel('Time \\n (Daily Samples in Chronological Order)')\n",
    "plt.ylabel('Precipitation Anomalies (mm/day)')\n",
    "plt.legend(markerscale = 22, loc = 'upper right')\n",
    "plt.ylim(-7, 22)\n",
    "\n",
    "plt.savefig('/Users/C830793391/Documents/Research/E3SM/visuals/scatter_ENSO_phases_V2.png', format='png', bbox_inches ='tight', dpi = 300)\n",
    "\n",
    "print(target[elnino].shape)\n",
    "\n",
    "# plt.figure(figsize=(13, 4))\n",
    "# plt.scatter(sample_index[neutral,0], target[neutral], alpha = 0.8, label = 'Neutral target Anomalies (mm/day)', s=0.1, color = '#bababa')\n",
    "# plt.scatter(sample_index[lanina,0], target[lanina], alpha = 0.8, label = 'La Nina target Anomalies (mm/day)', s=0.1, color = '#bc77d0')\n",
    "# plt.scatter(sample_index[elnino,0], target[elnino], alpha = 0.8, label = 'El Nino target Anomalies (mm/day)', s=0.1, color = '#499797')\n",
    "# # plt.scatter(index10P_anoms, narrow_10P_anoms, label='Narrowest 10% IQR Target Anomalies (mm/day)', s=1.5, color = '#5ec962')\n",
    "# # plt.scatter(elnino, elnino_dots, label='El Nino Events', s=1, color = '#fdca26')\n",
    "# plt.xlabel('Time \\n (Daily Samples in Chronological Order)')\n",
    "# plt.ylabel('Precipitation Anomalies (mm/day)')\n",
    "# plt.legend(markerscale = 9, loc = 'upper right')\n",
    "# plt.ylim(-7, 22)\n",
    "\n",
    "# plt.figure(figsize=(13, 4))\n",
    "# plt.scatter(sample_index[lanina,0], target[lanina], alpha = 0.8, label = 'La Nina target Anomalies (mm/day)', s=0.1, color = '#bc77d0')\n",
    "# plt.scatter(sample_index[elnino,0], target[elnino], alpha = 0.8, label = 'El Nino target Anomalies (mm/day)', s=0.1, color = '#499797')\n",
    "# plt.scatter(sample_index[neutral,0], target[neutral], alpha = 0.8, label = 'Neutral target Anomalies (mm/day)', s=0.1, color = '#bababa')\n",
    "# # plt.scatter(index10P_anoms, narrow_10P_anoms, label='Narrowest 10% IQR Target Anomalies (mm/day)', s=1.5, color = '#5ec962')\n",
    "# # plt.scatter(elnino, elnino_dots, label='El Nino Events', s=1, color = '#fdca26')\n",
    "# plt.xlabel('Time \\n (Daily Samples in Chronological Order)')\n",
    "# plt.ylabel('Precipitation Anomalies (mm/day)')\n",
    "# plt.legend(markerscale = 9, loc = 'upper right')\n",
    "# plt.ylim(-7, 22)\n",
    "\n",
    "print(f\"Mean Precip Anomaly during El Nino: {np.round(target[elnino].mean(), 4)}\")\n",
    "print(f\"Mean Precip Anomaly during La Nina: {np.round(target[lanina].mean(), 4)}\")\n",
    "print(f\"Mean Precip Anomaly during Neutral: {np.round(target[neutral].mean(), 4)}\")\n",
    "\n",
    "print(f\"Mean True Precip Amount during El Nino: {np.round(target_raw[elnino].mean(), 4)}\")\n",
    "print(f\"Mean True Precip Amount during La Nina: {np.round(target_raw[lanina].mean(), 4)}\")\n",
    "print(f\"Mean True Precip Amount during Neutral: {np.round(target_raw[neutral].mean(), 4)}\")\n",
    "\n",
    "print(f\"Percent of La Nina events out of total: {np.round((lanina.shape[0]/target.shape[0])*100, 2)}%\")\n",
    "print(f\"Percent of El Nino events out of total: {np.round((elnino.shape[0]/target.shape[0])*100, 2)}%\")\n",
    "print(f\"Percent of Neutral events out of total: {np.round((neutral.shape[0]/target.shape[0])*100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter Compare: \n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.scatter(sample_index[:,0], target_raw, label = 'All True Target Values (mm/day)', s=1.5, color = '#3b528b')\n",
    "plt.scatter(index10P_true, narrow_10P_true, label='Narrowest 10% IQR True Target Values (mm/day)', s=1.5, color = '#5ec962')\n",
    "plt.xlabel('Time \\n (Daily Samples in Chronological Order)')\n",
    "plt.ylabel('True Precipitation (mm/day)')\n",
    "plt.legend(loc = 'upper right')\n",
    "plt.ylim(-2, 55)\n",
    "\n",
    "plt.savefig('/Users/C830793391/Documents/Research/E3SM/visuals/narrowest10P_TRUE_scatter_compare.png', format='png', bbox_inches ='tight', dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recalculate avg_target using the method above and the actual target data: \n",
    "avg_target_raw = np.zeros(len(percentiles))\n",
    "for ip, p in enumerate(percentiles):\n",
    "    avg_target_raw[ip] = np.mean(target_raw[iqr < np.percentile(iqr, p)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replot with true target data rather than anomalies (mm/day)\n",
    "color = 'tab:blue'\n",
    "fig, ax1 = plt.subplots()\n",
    "plt.gca().invert_xaxis()\n",
    "ax1.set_ylabel('Bin-Averaged CRPS', color =color)\n",
    "ax1.set_xlabel('IQR Percentile (% Data Remaining)')\n",
    "ax1.plot(percentiles, avg_crps, color=color, linewidth = 1.8)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "ax1.axhline(y=crps_climatology_scores.mean(), color='grey', linestyle='--', label='Climatological Mean CRPS')\n",
    "ax1.legend(loc=('lower left'))\n",
    "ax1.set_ylim([1.05, 1.21])\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:purple'\n",
    "ax2.set_ylabel('Bin-Averaged Target Precip (mm/day)', color = color)\n",
    "ax2.plot(percentiles, avg_target_raw, color=color, linestyle=':', linewidth = 2)  \n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "plt.savefig('/Users/C830793391/Documents/Research/E3SM/visuals/CRPS_IQR_DiscardPlot_mean_raw_target_V2.png', format='png', bbox_inches ='tight', dpi = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CRPS with //median// rather than mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an array of averaged CRPS score for each bin of IQR percentile (100, 95, 90, 85, etc.)\n",
    "percentiles = np.linspace(100, 0, 101)\n",
    "\n",
    "median_crps = []\n",
    "median_target = []\n",
    "for p in percentiles:\n",
    "    # print(f\"Percentile data remaining is: {p}\")\n",
    "    # print(iqr < np.percentile(iqr, p))\n",
    "    median_crps.append(np.median(crps_scores[iqr < np.percentile(iqr, p)]))\n",
    "    median_target.append(np.median(target[iqr < np.percentile(iqr, p)]))\n",
    "\n",
    "color = 'tab:blue'\n",
    "fig, ax1 = plt.subplots()\n",
    "plt.gca().invert_xaxis()\n",
    "ax1.set_ylabel('Median CRPS')\n",
    "ax1.set_xlabel('IQR Percentile (% Data Remaining)', color=color)\n",
    "ax1.plot(percentiles, median_crps, color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "ax1.axhline(y= np.median(crps_climatology_scores), color='grey', linestyle='--', label='Climatology Median')\n",
    "ax1.set_ylim([0.7, 0.85])\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:olive'\n",
    "ax2.set_ylabel('Median Target Anomalies (mm/day)', color=color)\n",
    "ax2.plot(percentiles, median_target, color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "plt.savefig('/Users/C830793391/Documents/Research/E3SM/visuals/CRPS_IQR_DiscardPlot_fine_median.png', format='png', bbox_inches ='tight', dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the 10% narrowest IQR SHASH samples and plot the distributions relative to climatology\n",
    "narrowest = np.percentile(iqr, 5)\n",
    "narrowest_indices = np.where(iqr < narrowest)\n",
    "narrowest_output = output[narrowest_indices]\n",
    "\n",
    "x = np.arange(-50, 50, 0.0075)\n",
    "\n",
    "dist_narrow = Shash(narrowest_output)\n",
    "narrow_p = dist_narrow.prob(x).numpy()\n",
    "narrow_dist_targets = target[narrowest_indices]\n",
    "\n",
    "print(f\"There are {len(narrowest_indices[0])} samples in the narrowest 10% of IQR SHASH samples\")\n",
    "print(f\"The mean of targets corresponding to the narrowest 10% of IQR SHASH samples is: {round(target[narrowest_indices].mean(), 4)}\")\n",
    "print(f\"The mean of all the targets is: {np.round(target.mean(), 4)}\")\n",
    "\n",
    "percentile_15 = np.percentile(iqr, 15)\n",
    "percentile_20 = np.percentile(iqr, 20)\n",
    "indices_15_20 = np.where((iqr > percentile_15) & (iqr < percentile_20))\n",
    "print(f\"The mean of targets corresponding to the 15th to 20th (BEST CRPS) of IQR SHASH samples is: {round(target[indices_15_20].mean(), 4)}\")\n",
    "\n",
    "# Average network SHASH TODO: THIS SHOULD BE DONE USING SHASH.MEAN() METHOD\n",
    "average_SHASH = np.mean(output, axis=0)\n",
    "print(average_SHASH)\n",
    "# Average network SHASH output for narrowest 10% of IQR SHASH samples\n",
    "average_SHASH_narrow = np.mean(narrowest_output, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# narrow_dist_plot = deriveclimatology(narrowest_samples, cfile, x, testset)\n",
    "\n",
    "from shash import shash_torch\n",
    "\n",
    "cfile = '/Users/C830793391/BIG_DATA/E3SM_Data/presaved/Network Inputs/exp006_test.pkl'\n",
    "\n",
    "with gzip.open(cfile, \"rb\") as obj1:\n",
    "    data = pickle.load(obj1)\n",
    "climatology = data[\"y\"] # pulling all target values from processed data\n",
    "print(f\"Climatologial Mean = {np.mean(climatology)}\")\n",
    "print(f\"Climatological standard deviation = {np.std(climatology)}\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 4), dpi=200)\n",
    "plt.hist(\n",
    "    climatology, x, density=True, color=\"silver\", alpha=0.75, label=\"climatology\"\n",
    ")\n",
    "\n",
    "plt.plot(x, narrow_p, linewidth = 0.5, alpha = 0.005, color = 'green' ) \n",
    "plt.xlabel(\"value\")\n",
    "plt.ylabel(\"probability density\")\n",
    "plt.title(\"Network Shash Prediction\")\n",
    "# plt.axvline(valset[:len(output)], color='r', linestyle='dashed', linewidth=1)\n",
    "plt.legend()\n",
    "plt.savefig('/Users/C830793391/Documents/Research/E3SM/visuals/ens3/exp006_narrowest_predictions_w_climatology_V2.png', format='png', bbox_inches ='tight', dpi = 300)\n",
    "plt.xlim([-10, 10])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skill-Spread Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare SHASH output for analysis ----------------------------------\n",
    "tensor_list = []\n",
    "for i in range(output.shape[1]):\n",
    "    tensor_list.append(torch.from_numpy(output[:, i]))\n",
    "\n",
    "output_tensor = torch.stack(tensor_list, dim=1)\n",
    "\n",
    "output_shash_instance = Shash(output)\n",
    "network_std_tensor = output_shash_instance.std()\n",
    "network_mean_tensor = output_shash_instance.mean()\n",
    "\n",
    "# Convert back to numpy: \n",
    "network_std = network_std_tensor.numpy()\n",
    "network_mean = network_mean_tensor.numpy()\n",
    "\n",
    "print(f\"Mean; {network_mean}, Std: {network_std}\")\n",
    "\n",
    "network_rmse = np.sqrt( ((target - network_mean) **2) / len(target) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X-Axis: Standard Deviation of model's predicted distribution\n",
    "# Y-Axis: RMSE of model's predicted distribution\n",
    "# Compute spread skill ratio and compare to climatology's Spread-Skill Ratio\n",
    "\n",
    "## Climatology: \n",
    "climatology_mean = 0.00018667661056060916\n",
    "climatology_std = 2.186552186770753\n",
    "climatology_std = np.repeat(climatology_std, len(target))\n",
    "climatology_rmse = np.sqrt( ((target - climatology_mean) **2) / len(target) )\n",
    "\n",
    "num_bins = 12\n",
    "percentile_bins = np.percentile(network_std, np.linspace(0, 100, num_bins + 1))\n",
    "network_bin_indices = np.digitize(network_std, percentile_bins)\n",
    "\n",
    "# Calculate the mean values for each bin\n",
    "network_std_binned = []\n",
    "network_rmse_binned = []\n",
    "\n",
    "for i in range(1, num_bins + 1):\n",
    "    bin_mask = network_bin_indices == i\n",
    "    if np.any(bin_mask):\n",
    "        network_std_binned.append(network_std[bin_mask].mean())\n",
    "        network_rmse_binned.append(network_rmse[bin_mask].mean())\n",
    "\n",
    "# Repeat for climatology data\n",
    "climatology_bin_indices = np.digitize(climatology_std, percentile_bins)\n",
    "\n",
    "climatology_std_binned = []\n",
    "climatology_rmse_binned = []\n",
    "\n",
    "for i in range(1, num_bins + 1):\n",
    "    bin_mask = climatology_bin_indices == i\n",
    "    if np.any(bin_mask):\n",
    "        climatology_std_binned.append(climatology_std[bin_mask].mean())\n",
    "        climatology_rmse_binned.append(climatology_rmse[bin_mask].mean())\n",
    "\n",
    "# 1:1 reference line\n",
    "max_val = max(max(network_std_binned), max(network_rmse_binned), max(climatology_std_binned), max(climatology_rmse_binned))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot([0, max_val], [0, max_val], 'k--', color = 'gray', label='1:1 Reference Line')\n",
    "plt.plot(network_std_binned, network_rmse_binned, 'o-', markersize=5, label='Network')\n",
    "plt.plot(climatology_std_binned, climatology_rmse_binned, 'o-', markersize=5, label='Climatology')\n",
    "plt.xlabel('Spread (Standard Deviation)')\n",
    "plt.ylabel('Skill (RMSE)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# plt.savefig('/Users/C830793391/Documents/Research/E3SM/visuals/SpreadSkillRatio_network_vs_climatology.png', format='png', bbox_inches ='tight', dpi = 300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
